{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Members:\n",
    "- 109065511 張宜禎\n",
    "- 109062562 蔡哲維\n",
    "- 108065425 丘騏銘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "import imageio\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import re\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Restrict TensorFlow to only use the first GPU\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters and Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "BUFFER_SIZE = 20000\n",
    "DATASET_SIZE = 211485\n",
    "\n",
    "SAMPLE_COL = 8\n",
    "SAMPLE_ROW = 8\n",
    "SAMPLE_NUM = SAMPLE_COL * SAMPLE_ROW\n",
    "\n",
    "CAPTION_NUM = 70495\n",
    "\n",
    "IMAGE_HEIGHT = 64\n",
    "IMAGE_WIDTH = 64\n",
    "IMAGE_CHANNEL = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparas = {\n",
    "    'EMBED_DIM': 1024,                         # word embedding dimension\n",
    "    'Z_DIM': 256,                             # random noise z dimension\n",
    "    'DENSE_DIM': 128,                         # number of neurons in dense layer\n",
    "    'IMAGE_SIZE': [64, 64, 3],                # render image size\n",
    "    'BATCH_SIZE': 256,\n",
    "    'LR': 1e-4,\n",
    "    'LR_DECAY': 0.5,\n",
    "    'BETA_1': 0.5,\n",
    "    'N_EPOCH': 600,\n",
    "    'N_SAMPLE': DATASET_SIZE // BATCH_SIZE,          # size of training data\n",
    "    'rs_Train': float(BATCH_SIZE) / float(DATASET_SIZE), \n",
    "    'CHECKPOINTS_DIR': './checkpoints/train',  # checkpoint path\n",
    "    'PRINT_FREQ': 1,                       # printing frequency of loss\n",
    "    'BZ':(BATCH_SIZE,256),\n",
    "    'TEST_Z':(SAMPLE_NUM,256),\n",
    "    'TEST_BATCH_SIZE':91\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function\n",
    "def utPuzzle(imgs, row, col, path=None):\n",
    "    h, w, c = imgs[0].shape\n",
    "    out = np.zeros((h * row, w * col, c), np.uint8)\n",
    "    for n, img in enumerate(imgs):\n",
    "        j, i = divmod(n, col)\n",
    "        out[j * h : (j + 1) * h, i * w : (i + 1) * w, :] = img\n",
    "    if path is not None : imageio.imwrite(path, out)\n",
    "    return out\n",
    "  \n",
    "def utMakeGif(imgs, fname, duration):\n",
    "    n = float(len(imgs)) / duration\n",
    "    clip = mpy.VideoClip(lambda t : imgs[int(n * t)], duration = duration)\n",
    "    clip.write_gif(fname, fps = n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_path = './dictionary'\n",
    "vocab = np.load(dictionary_path + '/vocab.npy')\n",
    "print('there are {} vocabularies in total'.format(len(vocab)))\n",
    "\n",
    "word2Id_dict = dict(np.load(dictionary_path + '/word2Id.npy'))\n",
    "id2word_dict = dict(np.load(dictionary_path + '/id2Word.npy'))\n",
    "print('Word to id mapping, for example: %s -> %s' % ('flower', word2Id_dict['flower']))\n",
    "print('Id to word mapping, for example: %s -> %s' % ('1', id2word_dict['1']))\n",
    "print('Tokens: <PAD>: %s; <RARE>: %s' % (word2Id_dict['<PAD>'], word2Id_dict['<RARE>']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2IdList(line, MAX_SEQ_LENGTH=20):\n",
    "    MAX_SEQ_LIMIT = MAX_SEQ_LENGTH\n",
    "    padding = 0\n",
    "    \n",
    "    # data preprocessing, remove all puntuation in the texts\n",
    "    prep_line = re.sub('[%s]' % re.escape(string.punctuation), ' ', line.rstrip())\n",
    "    prep_line = prep_line.replace('-', ' ')\n",
    "    prep_line = prep_line.replace('-', ' ')\n",
    "    prep_line = prep_line.replace('  ', ' ')\n",
    "    prep_line = prep_line.replace('.', '')\n",
    "    tokens = prep_line.split(' ')\n",
    "    tokens = [\n",
    "        tokens[i] for i in range(len(tokens))\n",
    "        if tokens[i] != ' ' and tokens[i] != ''\n",
    "    ]\n",
    "    l = len(tokens)\n",
    "    padding = MAX_SEQ_LIMIT - l\n",
    "    \n",
    "    # make sure length of each text is equal to MAX_SEQ_LENGTH, and replace the less common word with <RARE> token\n",
    "    for i in range(padding):\n",
    "        tokens.append('<PAD>')\n",
    "    line = [\n",
    "        word2Id_dict[tokens[k]]\n",
    "        if tokens[k] in word2Id_dict else word2Id_dict['<RARE>']\n",
    "        for k in range(len(tokens))\n",
    "    ]\n",
    "\n",
    "    return line\n",
    "\n",
    "text = \"the flower shown has yellow anther red pistil and bright red petals.\"\n",
    "print(text)\n",
    "print(sent2IdList(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idx2word(indices_list):\n",
    "    results_list = []\n",
    "    for indices in indices_list:\n",
    "        string = ''\n",
    "        length_of_string = 0\n",
    "        for idx in indices:\n",
    "            if idx == '5428':\n",
    "                string = string + ''\n",
    "            elif idx == '5427':\n",
    "                break\n",
    "            else:\n",
    "                string = string + id2word_dict[idx] + ' '\n",
    "        results_list.append(string.strip())\n",
    "    return results_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './dataset'\n",
    "df = pd.read_pickle(data_path + '/text2ImgData.pkl')\n",
    "num_training_sample = len(df)\n",
    "n_images_train = num_training_sample\n",
    "print('There are %d image in training data' % (n_images_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['texts'] = df['Captions'].apply(lambda x: idx2word(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[0,'texts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_empty_string(string_list):\n",
    "    empty_flag = False\n",
    "    for string in string_list:\n",
    "        if string == '':\n",
    "            empty_flag = True\n",
    "            break\n",
    "    if empty_flag == False:\n",
    "        return string_list\n",
    "    else:\n",
    "        new_string_list = []\n",
    "        for string in string_list:\n",
    "            if string != '':\n",
    "                new_string_list.append(string)\n",
    "        return new_string_list       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['texts'] = df['texts'].apply(lambda x: remove_empty_string(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### See the number of captions of each image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_caption_num(string_list):\n",
    "    return len(string_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['caption_num'] = df['texts'].apply(lambda c: count_caption_num(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_dict = {}\n",
    "for num in df['caption_num'].tolist():\n",
    "    if num in num_dict:\n",
    "        num_dict[num]+=1\n",
    "    else:\n",
    "        num_dict[num]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get BERT Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFBertModel\n",
    "\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\n",
    "    'bert-large-uncased', \n",
    "    do_lower_case=False,\n",
    "    do_basic_tokenize=False\n",
    ")\n",
    "bert_model = TFBertModel.from_pretrained('bert-large-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def turn_to_bert_embedding(string_list):\n",
    "    try:\n",
    "        bert_inputs = bert_tokenizer(string_list, return_tensors=\"tf\", padding='max_length',max_length=30)\n",
    "        bert_outputs = bert_model(bert_inputs)\n",
    "        caption_embedding = bert_outputs.last_hidden_state[:,0]\n",
    "    except(ValueError):\n",
    "        print(string_list)\n",
    "    return caption_embedding.numpy().tolist()\n",
    "\n",
    "test_string = ['this flower is white and pink in color with petals that have small veins', 'the flower shown has a purple and white petal with white anther', 'the four heart shaped pink petals of this flower are striped with fuchsia and their centers are yellow and white']  \n",
    "print(len(turn_to_bert_embedding(test_string)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "print(\"{}, start infering.\".format(datetime.now()))\n",
    "df['embeddings'] = df['texts'].apply(lambda x : turn_to_bert_embedding(x))\n",
    "print(\"{}, end infering.\".format(datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.loc[0,'embeddings'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(\"./dataset/text2img_cls_embedding.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write images into tf record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"./dataset/text2img_cls_embedding.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _bytes_feature(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    if isinstance(value, type(tf.constant(0))):\n",
    "        value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _float_feature(value):\n",
    "    \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img(img_path):\n",
    "    raw_img = tf.io.read_file(img_path)\n",
    "    ##################################\n",
    "    #img = tf.image.decode_jpeg(raw_img, channels=3)\n",
    "    #img = tf.image.resize(img, size=[IMAGE_HEIGHT, IMAGE_WIDTH])\n",
    "    #img = tf.cast(img,tf.float32)\n",
    "    #img = img / 255.\n",
    "    ##################################\n",
    "    return raw_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_example(img):\n",
    "    feature = {\n",
    "        'img': _bytes_feature(img)\n",
    "    }\n",
    "    \n",
    "    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    \n",
    "    return example_proto.SerializeToString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_serialize_example(img):\n",
    "    tf_string = tf.py_function(\n",
    "        serialize_example,\n",
    "        [img],  # pass these args to the above function.\n",
    "        tf.string)      # the return type is `tf.string`.\n",
    "    return tf.reshape(tf_string, ()) # The result is a scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = df['ImagePath'].values\n",
    "\n",
    "image_paths = np.asarray(image_paths)\n",
    "\n",
    "assert image_paths.shape[0] == 7370"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITEMS_PER_FILE = 3000\n",
    "num=0\n",
    "for i in range(0,len(image_paths),ITEMS_PER_FILE):\n",
    "    write_record_dataset = tf.data.Dataset.from_tensor_slices(image_paths[i:i+ITEMS_PER_FILE])\n",
    "    write_record_dataset = write_record_dataset.map(load_img,num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    write_record_dataset = write_record_dataset.map(tf_serialize_example,num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    filename = f'train_{num:03d}_.tfrecord'\n",
    "    writer = tf.data.experimental.TFRecordWriter(filename)\n",
    "    writer.write(write_record_dataset)\n",
    "    num+=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read from tf record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = ['train_000_.tfrecord','train_001_.tfrecord','train_002_.tfrecord']\n",
    "raw_dataset_train = tf.data.TFRecordDataset(filenames)\n",
    "raw_dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_description = {\n",
    "    'img': tf.io.FixedLenFeature([], tf.string),\n",
    "}\n",
    "\n",
    "def _parse_function(example_proto):\n",
    "    parsed = tf.io.parse_single_example(example_proto, feature_description)\n",
    "    return parsed['img']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset_train = raw_dataset_train.map(_parse_function)\n",
    "raw_dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing(img):\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, size=[IMAGE_HEIGHT, IMAGE_WIDTH])\n",
    "    img = tf.cast(img,tf.float32)\n",
    "    img = img / 255.\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset_train = raw_dataset_train.map(processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_img = 0\n",
    "\n",
    "imgs = []\n",
    "for img in raw_dataset_train:\n",
    "    imgs.append(img.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(imgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in imgs:\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create tf Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_data_generator(img, embedding):\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    img.set_shape([IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNEL])\n",
    "    img = img*2. - 1.\n",
    "    embedding = tf.cast(embedding, tf.float32)\n",
    "\n",
    "    return img, embedding\n",
    "\n",
    "def flip_right_left_data_generator(img, embedding):\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    img.set_shape([IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNEL])\n",
    "    img = img*2. -1.\n",
    "    img = tf.image.flip_left_right(img)\n",
    "    embedding = tf.cast(embedding, tf.float32)\n",
    "\n",
    "    return img, embedding\n",
    "\n",
    "\n",
    "def adjust_brightness_data_generator(img, embedding):\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    img = tf.image.random_brightness(img, 0.2, 2)\n",
    "    img.set_shape([IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNEL])\n",
    "    img = img*2. -1.\n",
    "    img = tf.image.flip_up_down(img)\n",
    "    embedding = tf.cast(embedding, tf.float32)\n",
    "\n",
    "    return img, embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"./dataset/text2img_cls_embedding.pkl\")\n",
    "\n",
    "embeddings = df['embeddings'].values\n",
    "\n",
    "embedding = []\n",
    "\n",
    "img_for_dataset = []\n",
    "\n",
    "for i in range(len(embeddings)):\n",
    "    for emb in embeddings[i]:\n",
    "        embedding.append(emb)\n",
    "        img_for_dataset.append(imgs[i])\n",
    "embedding = np.asarray(embedding)\n",
    "img_for_dataset = np.asarray(img_for_dataset)\n",
    "\n",
    "assert embedding.shape[0] == img_for_dataset.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((img_for_dataset, embedding))\n",
    "dataset = dataset.map(training_data_generator,num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "flip_right_left_dataset = tf.data.Dataset.from_tensor_slices((img_for_dataset, embedding))\n",
    "flip_right_left_dataset = flip_right_left_dataset.map(flip_right_left_data_generator,num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "adjust_brightness_dataset = tf.data.Dataset.from_tensor_slices((img_for_dataset, embedding))\n",
    "adjust_brightness_dataset = adjust_brightness_dataset.map(adjust_brightness_data_generator,num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "dataset = dataset.concatenate(flip_right_left_dataset)\n",
    "dataset = dataset.concatenate(adjust_brightness_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 0\n",
    "for img, emb in dataset:\n",
    "    num+=1\n",
    "num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modified DCGAN with WGAN-GP Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Generate fake image based on given text(hidden representation) and noise z\n",
    "    input: text and noise\n",
    "    output: fake image with size 64*64*3\n",
    "    \"\"\"\n",
    "    def __init__(self, hparas):\n",
    "        super(Generator, self).__init__()\n",
    "        self.hparas = hparas\n",
    "        self.compress = tf.keras.layers.Dense(self.hparas['DENSE_DIM'])\n",
    "        self.to_4_4_1024 = tf.keras.layers.Dense(4*4*1024)\n",
    "        \n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "        self.bn3 = tf.keras.layers.BatchNormalization()\n",
    "        self.bn4 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.lr1 = tf.keras.layers.LeakyReLU()\n",
    "        self.lr2 = tf.keras.layers.LeakyReLU()\n",
    "        self.lr3 = tf.keras.layers.LeakyReLU()\n",
    "        self.lr4 = tf.keras.layers.LeakyReLU()\n",
    "        self.lr5 = tf.keras.layers.LeakyReLU()\n",
    "        \n",
    "        self.dc1 = tf.keras.layers.Conv2DTranspose(\n",
    "            filters = 512,\n",
    "            kernel_size = 5,\n",
    "            strides = 2,\n",
    "            padding = \"SAME\"\n",
    "        )\n",
    "        self.dc2 = tf.keras.layers.Conv2DTranspose(\n",
    "            filters = 256,\n",
    "            kernel_size = 5,\n",
    "            strides = 2,\n",
    "            padding = \"SAME\"\n",
    "        )\n",
    "        self.dc3 = tf.keras.layers.Conv2DTranspose(\n",
    "            filters = 128,\n",
    "            kernel_size = 5,\n",
    "            strides = 2,\n",
    "            padding = \"SAME\"\n",
    "        )\n",
    "        self.dc4 = tf.keras.layers.Conv2DTranspose(\n",
    "            filters = 3,\n",
    "            kernel_size = 5,\n",
    "            strides = 2,\n",
    "            padding = \"SAME\"\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def call(self, noise_z, text, training):\n",
    "        # compress the embedding\n",
    "        text = self.compress(text)\n",
    "        text = self.lr1(text)\n",
    "        \n",
    "        # concatenate input text and random noise\n",
    "        text_concat = tf.concat([noise_z, text], axis=1)\n",
    "        \n",
    "        # To 4*4*1024\n",
    "        text_concat = self.to_4_4_1024(text_concat)\n",
    "        text_concat = tf.reshape(text_concat, [-1, 4, 4, 1024])\n",
    "        text_concat = self.bn1(text_concat,training=training)\n",
    "        text_concat = self.lr2(text_concat)\n",
    "        \n",
    "        # To 8*8*512\n",
    "        text_concat = self.dc1(text_concat)\n",
    "        text_concat = self.bn2(text_concat,training=training)\n",
    "        text_concat = self.lr3(text_concat)\n",
    "        \n",
    "        # To 16*16*256\n",
    "        text_concat = self.dc2(text_concat)\n",
    "        text_concat = self.bn3(text_concat,training=training)\n",
    "        text_concat = self.lr4(text_concat)\n",
    "        \n",
    "        # To 32*32*128\n",
    "        text_concat = self.dc3(text_concat)\n",
    "        text_concat = self.bn4(text_concat,training=training)\n",
    "        text_concat = self.lr5(text_concat)\n",
    "        \n",
    "        # To 64*64*3\n",
    "        text_concat = self.dc4(text_concat)\n",
    "        \n",
    "        output = tf.nn.tanh(text_concat)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Differentiate the real and fake image\n",
    "    input: image and corresponding text\n",
    "    output: labels, the real image should be 1, while the fake should be 0\n",
    "    \"\"\"\n",
    "    def __init__(self, hparas):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.hparas = hparas\n",
    "        self.compress = tf.keras.layers.Dense(self.hparas['DENSE_DIM'])\n",
    "        self.d = tf.keras.layers.Dense(1)\n",
    "        \n",
    "        self.relu = tf.keras.layers.ReLU()\n",
    "        self.relu2 = tf.keras.layers.ReLU()\n",
    "        \n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "        self.bn3 = tf.keras.layers.BatchNormalization()\n",
    "        self.bn4 = tf.keras.layers.BatchNormalization()\n",
    "        self.bn5 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.lr1 = tf.keras.layers.LeakyReLU()\n",
    "        self.lr2 = tf.keras.layers.LeakyReLU()\n",
    "        self.lr3 = tf.keras.layers.LeakyReLU()\n",
    "        self.lr4 = tf.keras.layers.LeakyReLU()\n",
    "        self.lr5 = tf.keras.layers.LeakyReLU()\n",
    "        \n",
    "        \n",
    "        self.conv1 = tf.keras.layers.Conv2D(\n",
    "            filters = 128,\n",
    "            kernel_size = 5,\n",
    "            strides = (2, 2),\n",
    "            padding = \"SAME\",\n",
    "            input_shape = (64,64,3))\n",
    "        \n",
    "        self.conv2 = tf.keras.layers.Conv2D(\n",
    "            filters = 256,\n",
    "            kernel_size = 5,\n",
    "            strides = (2, 2),\n",
    "            padding = \"SAME\")\n",
    "        \n",
    "        self.conv3 = tf.keras.layers.Conv2D(\n",
    "            filters = 512,\n",
    "            kernel_size = 5,\n",
    "            strides = (2, 2),\n",
    "            padding = \"SAME\")\n",
    "        \n",
    "        self.conv4 = tf.keras.layers.Conv2D(\n",
    "            filters = 1024,\n",
    "            kernel_size = 5,\n",
    "            strides = (2, 2),\n",
    "            padding = \"SAME\")\n",
    "        \n",
    "        self.conv5 = tf.keras.layers.Conv2D(\n",
    "            filters = 1024,\n",
    "            kernel_size = 1,\n",
    "            strides = (1, 1),\n",
    "            padding = \"SAME\")\n",
    "    \n",
    "    def call(self, img, text, training):\n",
    "        # Conpress embedding\n",
    "        text = self.compress(text)\n",
    "        text = self.relu(text)\n",
    "        # To 32*32*128\n",
    "        img = self.conv1(img)\n",
    "        #img = self.bn1(img,training=training)\n",
    "        img = self.lr1(img)\n",
    "        # To 16*16*256\n",
    "        img = self.conv2(img)\n",
    "        #img = self.bn2(img,training=training)\n",
    "        img = self.lr2(img)\n",
    "        # To 8*8*512\n",
    "        img = self.conv3(img)\n",
    "        #img = self.bn3(img,training=training)\n",
    "        img = self.lr3(img)\n",
    "        # To 4*4*1024\n",
    "        img = self.conv4(img)\n",
    "        #img = self.bn4(img,training=training)\n",
    "        img = self.lr4(img)\n",
    "        \n",
    "        # concatenate image with paired text\n",
    "        text = tf.expand_dims(text,axis=1)\n",
    "        text = tf.expand_dims(text,axis=1)\n",
    "        text = tf.tile(text,multiples=[1,4,4,1])\n",
    "        img_text = tf.concat([img, text], axis=-1)\n",
    "        \n",
    "        img_text = self.conv5(img_text)\n",
    "        #img_text = self.bn5(img_text,training=training)\n",
    "        img_text = self.relu2(img_text)\n",
    "        \n",
    "        img_text = tf.reshape(img_text, [-1, 4*4*1024])\n",
    "        \n",
    "        \n",
    "        score = self.d(img_text)\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(hparas)\n",
    "discriminator = Discriminator(hparas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(hparas['LR'])\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(hparas['LR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = hparas['CHECKPOINTS_DIR']\n",
    "ckpt = tf.train.Checkpoint(generator = generator,\n",
    "                           discriminator = discriminator,\n",
    "                           generator_optimizer = generator_optimizer,\n",
    "                           discriminator_optimizer = discriminator_optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def DC_D_Train(c1,embed,noise_decay):\n",
    "    z = tf.random.normal(hparas['BZ']) \n",
    "\n",
    "    with tf.GradientTape() as tp:\n",
    "        with tf.GradientTape() as tp_2:\n",
    "            x_bar = generator(z, embed, training = True)\n",
    "            epsilon = tf.random.uniform([BATCH_SIZE,1,1,1])\n",
    "            x = c1\n",
    "            x_hat = epsilon * x + (1. - epsilon) * x_bar\n",
    "\n",
    "            x_bar = x_bar + noise_decay * tf.random.normal(x_bar.shape)\n",
    "            x = x + noise_decay * tf.random.normal(x.shape)\n",
    "            x_hat = x_hat + noise_decay * tf.random.normal(x_hat.shape)\n",
    "\n",
    "            z0 = discriminator(x_bar, embed, training = True)\n",
    "            z1 = discriminator(x, embed, training = True)\n",
    "            z2 = discriminator(x_hat, embed, training = True)\n",
    "\n",
    "            gradient_penalty = tp_2.gradient(z2,x_hat)\n",
    "            gradient_penalty = tf.sqrt(tf.reduce_sum(tf.math.square(gradient_penalty),axis=[1,2,3]))\n",
    "            loss = z0 - z1 + 10. * tf.math.square((gradient_penalty - 1.))\n",
    "            ld = tf.reduce_mean(loss)\n",
    "            lg = - tf.reduce_mean(z0)\n",
    "\n",
    "    gradient_d = tp.gradient(ld, discriminator.trainable_variables)\n",
    "\n",
    "    discriminator_optimizer.apply_gradients(zip(gradient_d, discriminator.trainable_variables))\n",
    "\n",
    "    return lg, ld\n",
    "\n",
    "@tf.function\n",
    "def DC_G_Train(c1,embed,noise_decay):\n",
    "    \n",
    "    z = tf.random.normal(hparas['BZ'])\n",
    "\n",
    "    with tf.GradientTape() as tp:\n",
    "        with tf.GradientTape() as tp_2:\n",
    "            x_bar = generator(z, embed, training = True)\n",
    "            epsilon = tf.random.uniform([BATCH_SIZE,1,1,1])\n",
    "            x = c1\n",
    "            x_hat = epsilon * x + (1. - epsilon) * x_bar\n",
    "\n",
    "            x_bar = x_bar + noise_decay * tf.random.normal(x_bar.shape)\n",
    "            x = x + noise_decay * tf.random.normal(x.shape)\n",
    "            x_hat = x_hat + noise_decay * tf.random.normal(x_hat.shape)\n",
    "\n",
    "            z0 = discriminator(x_bar, embed, training = True)\n",
    "            z1 = discriminator(x, embed, training = True)\n",
    "            z2 = discriminator(x_hat, embed, training = True)\n",
    "            gradient_penalty = tp_2.gradient(z2,x_hat)\n",
    "            gradient_penalty = tf.sqrt(tf.reduce_sum(tf.math.square(gradient_penalty),axis=[1,2,3]))\n",
    "            loss = z0 - z1 + 10. * tf.math.square((gradient_penalty - 1.))\n",
    "            ld = tf.reduce_mean(loss)\n",
    "            lg = - tf.reduce_mean(z0)\n",
    "\n",
    "    gradient_g = tp.gradient(lg, generator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradient_g, generator.trainable_variables))\n",
    "\n",
    "    return lg, ld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(noise, embed):\n",
    "    fake_image = generator(noise, embed, training = False)\n",
    "    return fake_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"./dataset/text2img_cls_embedding.pkl\")\n",
    "\n",
    "embeddings = df['embeddings'].values\n",
    "ids = df['Captions'].values\n",
    "\n",
    "test_embed = []\n",
    "\n",
    "for i in range(8):\n",
    "    if len(embeddings[i]) >= 8:\n",
    "        for j in range(8):\n",
    "            test_embed.append(embeddings[i][j])\n",
    "test_noise = tf.random.normal(hparas['TEST_Z'])\n",
    "test_embed = tf.Variable(test_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train = (\n",
    "    DC_D_Train,\n",
    "    DC_D_Train,\n",
    "    DC_D_Train,\n",
    "    DC_D_Train,\n",
    "    DC_D_Train,\n",
    "    DC_G_Train\n",
    ")\n",
    "\n",
    "Critic = len(Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wlg = [None] * hparas['N_EPOCH'] #record loss of g for each epoch\n",
    "wld = [None] * hparas['N_EPOCH']  #record loss of d for each epoch\n",
    "wsp = [None] * hparas['N_EPOCH']  #record sample images for each epoch\n",
    "\n",
    "rsTrain = hparas['rs_Train']\n",
    "ctr = 0\n",
    "for ep in range(hparas['N_EPOCH']):\n",
    "    print(\"Epoch: \" + str(ep+1), end='\\r')\n",
    "    print('')\n",
    "    lgt = 0.0\n",
    "    ldt = 0.0\n",
    "    if ep < 200:\n",
    "        noise_decay = 1.0 / float(ep+1)\n",
    "    else:\n",
    "        noise_decay = 0.0\n",
    "        \n",
    "    for idx, (real_img,embed) in enumerate(dataset):\n",
    "        print(str(idx+1) + '/' + str(hparas['N_SAMPLE']), end='\\r')\n",
    "        lg, ld = Train[ctr](real_img, embed, noise_decay)\n",
    "        ctr += 1\n",
    "        lgt += lg.numpy()\n",
    "        ldt += ld.numpy()\n",
    "        if ctr == Critic : ctr = 0\n",
    "    print('')\n",
    "    wlg[ep] = lgt * rsTrain\n",
    "    wld[ep] = ldt * rsTrain\n",
    "    with open('./wlg_v2.txt','a') as f:\n",
    "        f.write(str(lgt * rsTrain) + '\\n')\n",
    "    f.close()\n",
    "    with open('./wld_v2.txt','a') as f:\n",
    "        f.write(str(ldt * rsTrain) + '\\n')\n",
    "    f.close()\n",
    "    \n",
    "    out = test_step(test_noise, test_embed)\n",
    "    img = utPuzzle(\n",
    "        ((out+1) / 2. * 255.0).numpy().astype(np.uint8),\n",
    "        SAMPLE_COL,\n",
    "        SAMPLE_ROW,\n",
    "        \"imgs_v2/w_%04d.png\" % ep\n",
    "    )\n",
    "    wsp[ep] = img\n",
    "    if (ep+1) % 10 == 0: \n",
    "        plt.imshow(img)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(\"Epoch %d\" % (ep+1))\n",
    "        plt.show()\n",
    "    if (ep+1) % 10 == 0: \n",
    "        ckpt_manager.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
