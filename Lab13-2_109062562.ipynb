{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# You'll generate plots of attention in order to see which parts of an image\n",
    "# our model focuses on during captioning\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scikit-learn includes many helpful utilities\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Restrict TensorFlow to only use the first GPU\n",
    "        tf.config.experimental.set_visible_devices(gpus[2], 'GPU')\n",
    "\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 40\n",
    "SHUFFLE_BUFFER_SIZE = 5000\n",
    "embedding_dim = 256\n",
    "units = 512\n",
    "IMAGE_HEIGHT = 300\n",
    "IMAGE_WIDTH = 160\n",
    "LEARNING_RATE = 5e-5\n",
    "IMAGE_DIR = './words_captcha/'\n",
    "annotation_file = './words_captcha/spec_train_val.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the annotation txt file, and split into training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(annotation_file, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "f.close()\n",
    "\n",
    "train_img_name = []\n",
    "val_img_name = []\n",
    "train_annotation = []\n",
    "val_annotation = []\n",
    "num = 0\n",
    "\n",
    "for line in lines:\n",
    "    line = line.strip('\\n')\n",
    "    line = line.split(' ')\n",
    "    if num<100000:\n",
    "        train_img_name.append(line[0])\n",
    "        train_annotation.append(line[1])\n",
    "    else:\n",
    "        val_img_name.append(line[0])\n",
    "        val_annotation.append(line[1])\n",
    "    num+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 20000\n",
      "a0 thus\n",
      "a100000 cio\n"
     ]
    }
   ],
   "source": [
    "print(len(train_img_name), len(val_img_name))\n",
    "print(train_img_name[0] , train_annotation[0])\n",
    "print(val_img_name[0], val_annotation[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the character_to_index and index_to_character dictionary, and map the origin word to list of index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(annotations):\n",
    "    max_len = 0\n",
    "    for annotation in annotations:\n",
    "        if len(annotation) > max_len:\n",
    "            max_len = len(annotation)\n",
    "    return max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_to_idx = {}\n",
    "idx_to_character = {}\n",
    "character_to_idx['<pad>'] = 0\n",
    "idx_to_character[0] = '<pad>'\n",
    "index = 1\n",
    "\n",
    "for annotation in (train_annotation):\n",
    "    for character in annotation:\n",
    "        if character not in character_to_idx:\n",
    "            character_to_idx[character] = index\n",
    "            idx_to_character[index] = character\n",
    "            index+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<pad>': 0,\n",
       " 't': 1,\n",
       " 'h': 2,\n",
       " 'u': 3,\n",
       " 's': 4,\n",
       " 'w': 5,\n",
       " 'i': 6,\n",
       " 'e': 7,\n",
       " 'd': 8,\n",
       " 'j': 9,\n",
       " 'a': 10,\n",
       " 'm': 11,\n",
       " 'z': 12,\n",
       " 'o': 13,\n",
       " 'p': 14,\n",
       " 'l': 15,\n",
       " 'b': 16,\n",
       " 'g': 17,\n",
       " 'v': 18,\n",
       " 'k': 19,\n",
       " 'y': 20,\n",
       " 'n': 21,\n",
       " 'r': 22,\n",
       " 'c': 23,\n",
       " 'q': 24,\n",
       " 'f': 25,\n",
       " 'x': 26}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "character_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_to_idx['<start>'] = 27\n",
    "idx_to_character[27] = '<start>'\n",
    "\n",
    "character_to_idx['<end>'] = 28\n",
    "idx_to_character[28] = '<end>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<pad>': 0,\n",
       " 't': 1,\n",
       " 'h': 2,\n",
       " 'u': 3,\n",
       " 's': 4,\n",
       " 'w': 5,\n",
       " 'i': 6,\n",
       " 'e': 7,\n",
       " 'd': 8,\n",
       " 'j': 9,\n",
       " 'a': 10,\n",
       " 'm': 11,\n",
       " 'z': 12,\n",
       " 'o': 13,\n",
       " 'p': 14,\n",
       " 'l': 15,\n",
       " 'b': 16,\n",
       " 'g': 17,\n",
       " 'v': 18,\n",
       " 'k': 19,\n",
       " 'y': 20,\n",
       " 'n': 21,\n",
       " 'r': 22,\n",
       " 'c': 23,\n",
       " 'q': 24,\n",
       " 'f': 25,\n",
       " 'x': 26,\n",
       " '<start>': 27,\n",
       " '<end>': 28}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "character_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find out the max_length\n",
    "max_len = max_length(train_annotation) + 2\n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_annotation_idx = []\n",
    "val_annotation_idx = []\n",
    "\n",
    "for annotation in train_annotation:\n",
    "    annotation_idx = [27]\n",
    "    for character in annotation:\n",
    "        annotation_idx.append(character_to_idx[character])\n",
    "    annotation_idx.append(28)\n",
    "    while len(annotation_idx) < max_len:\n",
    "        annotation_idx.append(0)\n",
    "    train_annotation_idx.append(annotation_idx)\n",
    "    \n",
    "for annotation in val_annotation:\n",
    "    annotation_idx = [27]\n",
    "    for character in annotation:\n",
    "        annotation_idx.append(character_to_idx[character])\n",
    "    annotation_idx.append(28)\n",
    "    while len(annotation_idx) < max_len:\n",
    "        annotation_idx.append(0)\n",
    "    val_annotation_idx.append(annotation_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[27, 1, 2, 3, 4, 28, 0],\n",
       " [27, 5, 5, 5, 28, 0, 0],\n",
       " [27, 1, 6, 7, 8, 28, 0],\n",
       " [27, 6, 8, 4, 28, 0, 0],\n",
       " [27, 9, 10, 11, 28, 0, 0]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_annotation_idx[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[27, 23, 6, 13, 28, 0, 0],\n",
       " [27, 17, 11, 1, 28, 0, 0],\n",
       " [27, 15, 6, 18, 7, 4, 28],\n",
       " [27, 4, 2, 13, 5, 21, 28],\n",
       " [27, 4, 19, 20, 28, 0, 0]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_annotation_idx[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the train dataset and validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_name, annotation):\n",
    "    img = tf.io.read_file(IMAGE_DIR + image_name + '.png')\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, (IMAGE_HEIGHT, IMAGE_WIDTH))\n",
    "    img = img/255 - 1.\n",
    "    return img, annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_img_name,train_annotation_idx))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_img_name,val_annotation_idx))\n",
    "\n",
    "train_dataset = train_dataset.map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "val_dataset = val_dataset.map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE)\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE)\n",
    "train_dataset = train_dataset.prefetch(200)\n",
    "\n",
    "val_dataset = val_dataset.shuffle(SHUFFLE_BUFFER_SIZE)\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE)\n",
    "val_dataset = val_dataset.prefetch(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((None, 300, 160, 3), (None, 7)), types: (tf.float32, tf.int32)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(character_to_idx)\n",
    "num_steps = len(train_img_name) // BATCH_SIZE\n",
    "val_num_steps = len(val_img_name) // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n",
    "\n",
    "        # hidden shape == (batch_size, hidden_size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "\n",
    "        # score shape == (batch_size, 64, hidden_size)\n",
    "        score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
    "\n",
    "        # attention_weights shape == (batch_size, 64, 1)\n",
    "        # you get 1 at the last axis because you are applying score to self.V\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design feature extracter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv_relu(tf.keras.layers.Layer):\n",
    "    def __init__(self, filters, size, stride):\n",
    "        super(conv_relu, self).__init__()\n",
    "        self.conv = tf.keras.layers.Conv2D(filters, size, stride, padding=\"same\",\n",
    "                      kernel_initializer=tf.keras.initializers.TruncatedNormal())\n",
    "        self.batchnorm = tf.keras.layers.BatchNormalization()\n",
    "        self.lkrelu = tf.keras.layers.LeakyReLU(0.1)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        x = self.conv(inputs)\n",
    "        x = self.batchnorm(x,training = training)\n",
    "        x = self.lkrelu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature_Extracter(tf.keras.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Feature_Extracter, self).__init__()\n",
    "        self.cr1 = conv_relu(64,3,1)\n",
    "        self.cr2 = conv_relu(64,3,1)\n",
    "        self.max_pooling1 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2))\n",
    "        self.cr3 = conv_relu(128,3,1)\n",
    "        self.cr4 = conv_relu(128,3,1)\n",
    "        self.max_pooling2 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2))\n",
    "        self.cr5 = conv_relu(256,3,1)\n",
    "        self.cr6 = conv_relu(256,3,1)\n",
    "        self.cr7 = conv_relu(256,3,1)\n",
    "        self.max_pooling3 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2))\n",
    "        self.cr8 = conv_relu(512,3,1)\n",
    "        self.cr9 = conv_relu(512,3,1)\n",
    "        self.cr10 = conv_relu(512,3,1)\n",
    "        self.max_pooling4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2))\n",
    "        self.cr11 = conv_relu(512,3,1)\n",
    "        self.cr12 = conv_relu(512,3,1)\n",
    "        self.cr13 = conv_relu(512,3,1)\n",
    "        self.max_pooling5 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2))\n",
    "        self.cr14 = conv_relu(1024,3,1)\n",
    "        self.cr15 = conv_relu(1024,3,1)\n",
    "        self.cr16 = conv_relu(1024,3,1)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        x = self.cr1(inputs,training)\n",
    "        x = self.cr2(x,training)\n",
    "        x = self.max_pooling1(x)\n",
    "        x = self.cr3(x,training)\n",
    "        x = self.cr4(x,training)\n",
    "        x = self.max_pooling2(x)\n",
    "        x = self.cr5(x,training)\n",
    "        x = self.cr6(x,training)\n",
    "        x = self.cr7(x,training)\n",
    "        x = self.max_pooling3(x)\n",
    "        x = self.cr8(x,training)\n",
    "        x = self.cr9(x,training)\n",
    "        x = self.cr10(x,training)\n",
    "        x = self.max_pooling4(x)\n",
    "        x = self.cr11(x,training)\n",
    "        x = self.cr12(x,training)\n",
    "        x = self.cr13(x,training)\n",
    "        x = self.max_pooling5(x)\n",
    "        x = self.cr14(x,training)\n",
    "        x = self.cr15(x,training)\n",
    "        x = self.cr16(x,training)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Encoder(tf.keras.Model):\n",
    "    # Since you have already extracted the features and dumped it using pickle\n",
    "    # This encoder passes those features through a Fully connected layer\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        # shape after fc == (batch_size, 64, embedding_dim)\n",
    "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"feature__extracter\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv_relu (conv_relu)        multiple                  2048      \n",
      "_________________________________________________________________\n",
      "conv_relu_1 (conv_relu)      multiple                  37184     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv_relu_2 (conv_relu)      multiple                  74368     \n",
      "_________________________________________________________________\n",
      "conv_relu_3 (conv_relu)      multiple                  148096    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv_relu_4 (conv_relu)      multiple                  296192    \n",
      "_________________________________________________________________\n",
      "conv_relu_5 (conv_relu)      multiple                  591104    \n",
      "_________________________________________________________________\n",
      "conv_relu_6 (conv_relu)      multiple                  591104    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv_relu_7 (conv_relu)      multiple                  1182208   \n",
      "_________________________________________________________________\n",
      "conv_relu_8 (conv_relu)      multiple                  2361856   \n",
      "_________________________________________________________________\n",
      "conv_relu_9 (conv_relu)      multiple                  2361856   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv_relu_10 (conv_relu)     multiple                  2361856   \n",
      "_________________________________________________________________\n",
      "conv_relu_11 (conv_relu)     multiple                  2361856   \n",
      "_________________________________________________________________\n",
      "conv_relu_12 (conv_relu)     multiple                  2361856   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv_relu_13 (conv_relu)     multiple                  4723712   \n",
      "_________________________________________________________________\n",
      "conv_relu_14 (conv_relu)     multiple                  9442304   \n",
      "_________________________________________________________________\n",
      "conv_relu_15 (conv_relu)     multiple                  9442304   \n",
      "=================================================================\n",
      "Total params: 38,339,904\n",
      "Trainable params: 38,325,312\n",
      "Non-trainable params: 14,592\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "feature_extracter = Feature_Extracter()\n",
    "feature_extracter.build((None,IMAGE_HEIGHT,IMAGE_WIDTH,3))\n",
    "feature_extracter.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = CNN_Encoder(embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Decoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, units, vocab_size):\n",
    "        super(RNN_Decoder, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc1 = tf.keras.layers.Dense(self.units)\n",
    "        self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        self.attention = BahdanauAttention(self.units)\n",
    "\n",
    "    def call(self, x, features, hidden):\n",
    "        # defining attention as a separate model\n",
    "        context_vector, attention_weights = self.attention(features, hidden)\n",
    "\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "        # shape == (batch_size, max_length, hidden_size)\n",
    "        x = self.fc1(output)\n",
    "\n",
    "        # x shape == (batch_size * max_length, hidden_size)\n",
    "        x = tf.reshape(x, (-1, x.shape[2]))\n",
    "\n",
    "        # output shape == (batch_size * max_length, vocab)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x, state, attention_weights\n",
    "\n",
    "    def reset_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = RNN_Decoder(embedding_dim, units, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(LEARNING_RATE)\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"./checkpoints/train_v4\"\n",
    "ckpt = tf.train.Checkpoint(feature_extracter=feature_extracter,\n",
    "                           encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           optimizer = optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding this in a separate cell because if you run the training cell\n",
    "# many times, the loss_plot array will be reset\n",
    "loss_plot = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_shape = 1024\n",
    "attention_features_shape = 45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(img_tensor, target):\n",
    "    loss = 0\n",
    "\n",
    "    # initializing the hidden state for each batch\n",
    "    # because the captions are not related from image to image\n",
    "    hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "\n",
    "    dec_input = tf.expand_dims([character_to_idx['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        features = feature_extracter(img_tensor,True)\n",
    "        features = tf.reshape(features,(features.shape[0], -1, features.shape[3]))\n",
    "        features = encoder(features)\n",
    "\n",
    "        for i in range(1, target.shape[1]):\n",
    "            # passing the features through the decoder\n",
    "            predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "\n",
    "            loss += loss_function(target[:, i], predictions)\n",
    "\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "    total_loss = (loss / int(target.shape[1]))\n",
    "\n",
    "    trainable_variables = feature_extracter.trainable_variables + encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "    return loss, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 2500/2500 Train Loss 1.108733\n",
      "Validation Accuracy 0.431200, Validation Loss 0.390321\n",
      "Epoch 1 Train Loss 1.108733 Validation Accuracy 0.431200\n",
      "Time taken for 1 epoch 2970.0059263706207 sec\n",
      "\n",
      "Epoch 2 2500/2500 Train Loss 0.092519\n",
      "Validation Accuracy 0.862950, Validation Loss 0.077941\n",
      "Epoch 2 Train Loss 0.092519 Validation Accuracy 0.862950\n",
      "Time taken for 1 epoch 1035.8364934921265 sec\n",
      "\n",
      "Epoch 3 2500/2500 Train Loss 0.031428\n",
      "Validation Accuracy 0.837150, Validation Loss 0.088365\n",
      "Epoch 3 Train Loss 0.031428 Validation Accuracy 0.837150\n",
      "Time taken for 1 epoch 1032.3557267189026 sec\n",
      "\n",
      "Epoch 4 2500/2500 Train Loss 0.016177\n",
      "Validation Accuracy 0.913550, Validation Loss 0.053589\n",
      "Epoch 4 Train Loss 0.016177 Validation Accuracy 0.913550\n",
      "Time taken for 1 epoch 1032.164873123169 sec\n",
      "\n",
      "Epoch 5 2500/2500 Train Loss 0.012634\n",
      "Validation Accuracy 0.546050, Validation Loss 0.393007\n",
      "Epoch 5 Train Loss 0.012634 Validation Accuracy 0.546050\n",
      "Time taken for 1 epoch 1039.4833495616913 sec\n",
      "\n",
      "Epoch 6 2500/2500 Train Loss 0.009568\n",
      "Validation Accuracy 0.850300, Validation Loss 0.089063\n",
      "Epoch 6 Train Loss 0.009568 Validation Accuracy 0.850300\n",
      "Time taken for 1 epoch 1037.3173224925995 sec\n",
      "\n",
      "Epoch 7 2500/2500 Train Loss 0.007518\n",
      "Validation Accuracy 0.940100, Validation Loss 0.036169\n",
      "Epoch 7 Train Loss 0.007518 Validation Accuracy 0.940100\n",
      "Time taken for 1 epoch 1037.0093109607697 sec\n",
      "\n",
      "Epoch 8 2500/2500 Train Loss 0.006567\n",
      "Validation Accuracy 0.946050, Validation Loss 0.030031\n",
      "Epoch 8 Train Loss 0.006567 Validation Accuracy 0.946050\n",
      "Time taken for 1 epoch 1042.5231289863586 sec\n",
      "\n",
      "Epoch 9 2500/2500 Train Loss 0.006054\n",
      "Validation Accuracy 0.974600, Validation Loss 0.020613\n",
      "Epoch 9 Train Loss 0.006054 Validation Accuracy 0.974600\n",
      "Time taken for 1 epoch 1030.6967861652374 sec\n",
      "\n",
      "Epoch 10 2500/2500 Train Loss 0.004392\n",
      "Validation Accuracy 0.956500, Validation Loss 0.030154\n",
      "Epoch 10 Train Loss 0.004392 Validation Accuracy 0.956500\n",
      "Time taken for 1 epoch 1028.5976436138153 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "    #total_val_loss = 0\n",
    "\n",
    "    for (batch, (img_tensor, target)) in enumerate(train_dataset):\n",
    "        batch_loss, t_loss = train_step(img_tensor, target)\n",
    "        total_loss += t_loss\n",
    "        print ('Epoch {} {}/{} Train Loss {:.6f}'.format(epoch + 1,batch+1,num_steps,total_loss/(batch+1)),end='\\r')\n",
    "    print('')\n",
    "    equal_num = 0\n",
    "    total_val_loss = 0\n",
    "    for (batch, (img_tensor, target)) in enumerate(val_dataset):\n",
    "        val_loss = 0\n",
    "        hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "        dec_input = tf.expand_dims([character_to_idx['<start>']]*BATCH_SIZE, 1)\n",
    "        features = feature_extracter(img_tensor,False)\n",
    "        features = tf.reshape(features,(features.shape[0], -1, features.shape[3]))\n",
    "        features = encoder(features)\n",
    "        result = np.full((BATCH_SIZE, 1), 27)\n",
    "        for i in range(1, target.shape[1]):\n",
    "            # passing the features through the decoder\n",
    "            predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "            predicted_id = tf.argmax(predictions,axis=1).numpy()\n",
    "            val_loss += loss_function(target[:, i], predictions)\n",
    "            result = np.concatenate((result, predicted_id.reshape((BATCH_SIZE,1))), axis=1)\n",
    "            dec_input = tf.expand_dims(predicted_id, 1)\n",
    "        target_array = target.numpy()\n",
    "        total_val_loss += (val_loss / int(target.shape[1]))\n",
    "        for i in range(BATCH_SIZE):\n",
    "            for j in range(max_len):\n",
    "                if result[i][j] == 28 and target_array[i][j] == 28:\n",
    "                    if (result[i][1:j] == target_array[i][1:j]).all():\n",
    "                        equal_num+=1\n",
    "                    break\n",
    "        print ('Validation Accuracy {:.6f}, Validation Loss {:.6f}'.format(float(equal_num)/((batch+1)*BATCH_SIZE),total_val_loss/(batch+1)),end='\\r')\n",
    "    \n",
    "    print('')\n",
    "#         if batch % 100 == 0:\n",
    "#             print ('Epoch {} Batch {} Loss {:.4f}'.format(\n",
    "#               epoch + 1, batch, batch_loss.numpy() / int(target.shape[1])))\n",
    "    # storing the epoch end loss value to plot later\n",
    "    loss_plot.append(total_loss / num_steps)\n",
    "\n",
    "    ckpt_manager.save()\n",
    "    output_string = 'Epoch {} Train Loss {:.6f} Validation Accuracy {:.6f} Validation Loss {:.6f}\\n'.format(epoch + 1,\n",
    "                                                             total_loss/num_steps,float(equal_num)/20000.,total_val_loss/val_num_steps)\n",
    "    with open('./lab13-2_v4.log','a') as f:\n",
    "        f.write(output_string)\n",
    "    f.close()\n",
    "    print ('Epoch {} Train Loss {:.6f} Validation Accuracy {:.6f}'.format(epoch + 1,\n",
    "                                                             total_loss/num_steps,float(equal_num)/20000.))\n",
    "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restore ckpt9 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f11c86b4430>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt.restore('./checkpoints/train_v4/ckpt-9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy 0.974600, Validation Loss 0.020613\r"
     ]
    }
   ],
   "source": [
    "equal_num = 0\n",
    "total_val_loss = 0\n",
    "for (batch, (img_tensor, target)) in enumerate(val_dataset):\n",
    "    val_loss = 0\n",
    "    hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "    dec_input = tf.expand_dims([character_to_idx['<start>']]*BATCH_SIZE, 1)\n",
    "    features = feature_extracter(img_tensor,False)\n",
    "    features = tf.reshape(features,(features.shape[0], -1, features.shape[3]))\n",
    "    features = encoder(features)\n",
    "    result = np.full((BATCH_SIZE, 1), 27)\n",
    "    for i in range(1, target.shape[1]):\n",
    "        # passing the features through the decoder\n",
    "        predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "        predicted_id = tf.argmax(predictions,axis=1).numpy()\n",
    "        val_loss += loss_function(target[:, i], predictions)\n",
    "        result = np.concatenate((result, predicted_id.reshape((BATCH_SIZE,1))), axis=1)\n",
    "        dec_input = tf.expand_dims(predicted_id, 1)\n",
    "    target_array = target.numpy()\n",
    "    total_val_loss += (val_loss / int(target.shape[1]))\n",
    "    for i in range(BATCH_SIZE):\n",
    "        for j in range(max_len):\n",
    "            if result[i][j] == 28 and target_array[i][j] == 28:\n",
    "                if (result[i][1:j] == target_array[i][1:j]).all():\n",
    "                    equal_num+=1\n",
    "                break\n",
    "    print ('Validation Accuracy {:.6f}, Validation Loss {:.6f}'.format(float(equal_num)/((batch+1)*BATCH_SIZE),total_val_loss/(batch+1)),end='\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n"
     ]
    }
   ],
   "source": [
    "test_img_name = []\n",
    "\n",
    "for i in range(120000,140000):\n",
    "    test_img_name.append('a'+str(i))\n",
    "\n",
    "print(len(test_img_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_image(image_name):\n",
    "    img = tf.io.read_file(IMAGE_DIR + image_name + '.png')\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, (IMAGE_HEIGHT, IMAGE_WIDTH))\n",
    "    img = img/255 - 1.\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = tf.data.Dataset.from_tensor_slices(test_img_name)\n",
    "test_dataset = test_dataset.map(load_test_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)\n",
    "test_dataset = test_dataset.prefetch(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: (None, 300, 160, 3), types: tf.float32>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12117\n",
      "12996\n",
      "13104\n",
      "20000\n"
     ]
    }
   ],
   "source": [
    "num=0\n",
    "for batch, img_tensor in enumerate(test_dataset):\n",
    "    hidden = decoder.reset_state(batch_size=BATCH_SIZE)\n",
    "    dec_input = tf.expand_dims([character_to_idx['<start>']]*BATCH_SIZE, 1)\n",
    "    features = feature_extracter(img_tensor,False)\n",
    "    features = tf.reshape(features,(features.shape[0], -1, features.shape[3]))\n",
    "    features = encoder(features)\n",
    "    result = np.full((BATCH_SIZE, 1), 27)\n",
    "    for i in range(1, max_len):\n",
    "        # passing the features through the decoder\n",
    "        predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "        predicted_id = tf.argmax(predictions,axis=1).numpy()\n",
    "        result = np.concatenate((result, predicted_id.reshape((BATCH_SIZE,1))), axis=1)\n",
    "        dec_input = tf.expand_dims(predicted_id, 1)\n",
    "    for i in range(BATCH_SIZE):\n",
    "        output_str = ''\n",
    "        num = num+1\n",
    "        hit = False\n",
    "        for j in range(1,max_len):\n",
    "            if result[i][j] == 28:\n",
    "                hit = True\n",
    "                break\n",
    "            else:\n",
    "                output_str = output_str + idx_to_character[result[i][j]]\n",
    "        if hit != True:\n",
    "            print(num)\n",
    "        with open('./Lab13-2_109062562.txt','a') as f:\n",
    "            f.write('a' + str(119999 + num) + ' ' + output_str+'\\n')\n",
    "        f.close()\n",
    "print(num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess\n",
    "\n",
    "In the preprocess part, I first read in the \\'spec_train_val.txt\\' and split image names and annotations into train and validation list.\n",
    "\n",
    "Later, I create to dictionary, named character_to_idx and idx_to_character, to transfer the annotations into indices, character by character. At the same time, concate the \\'start\\' and \\'end\\' indices, and padding 0, which represent       \\'padding\\' at the end of annotation index sequences which are shorter than 7 (max sequence length).\n",
    "\n",
    "#### Reading the images\n",
    "\n",
    "When reading the images, I first resize the image size into 300\\*160 (to preserve the original ratio), and then convert the image channel value into -1~1.\n",
    "\n",
    "#### CNN model\n",
    "\n",
    "I design my model architecture based on CNN layers from vgg16 and add the Batchnormalization layers between CNN layers and activation layers (for the activation layers, I use leaky relu). After the 13 convolution layers, I add three more convolution layers which contain 1024 filters.\n",
    "\n",
    "#### Training\n",
    "\n",
    "In the train_step, I first send the images into CNN model I design, and then reshape the output into (None,45,1024). Then, the reshaped result is sent into CNN_Encoder to get the embedding.\n",
    "\n",
    "Later on send the embedding into the decoder to gain the prediction, and calculate loss function on the prediction and true value.\n",
    "\n",
    "The worth mention part is that, when doing the validation, we can't use teacher forcing mechanism, so I change the decoder input from target\\[:,i\\] to tf.expand_dims(tf.argmax(predictions,axis=1).numpy(),1), the indices the model predicted.\n",
    "\n",
    "The model have best validation accuracy and loss at 9th epoch. The accuracy is 0.97 and the loss is 0.020613. I use the weight to make prediction on test data.\n",
    "\n",
    "#### Problem I encountered, and how I solve it\n",
    "\n",
    "In my experiments, I actually encounter that my training loss will stuck at 2. Later on, I find that the reason is because the learning rate I use is too big, which is 0.001. It makes the model swing around the optimal spot.\n",
    "\n",
    "I then slove it by using the smaller learning rate, which is 5e-5.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
